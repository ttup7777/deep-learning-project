{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "np.random.seed(42)\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Input, LSTM, Bidirectional, Embedding, TimeDistributed, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "import nltk\n",
    "import re\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE =  'D:/Tudelft/q3/deep learning/project/glove.6B.200d.txt'\n",
    "train = pd.read_csv('D:/Tudelft/q3/deep learning/project/train.csv')\n",
    "test = pd.read_csv('D:/Tudelft/q3/deep learning/project/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "e8bd3575-f711-4ca6-a653-8ec1c74c0204",
    "_uuid": "cf43ac37cbd14d8baa088648c2275123550135d6"
   },
   "outputs": [],
   "source": [
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "109e2eaa-3e0a-4946-8d5b-f2e280b6cf82",
    "_uuid": "52d244d1cc11be42e0fafcebf67efae787dc0839"
   },
   "outputs": [],
   "source": [
    "X_train = list(X_train)\n",
    "X_test = list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79aff1ae-b342-47d7-a912-24418b96e0e7",
    "_uuid": "f5076a2f3496e0fa1baf7e095724132e45f9ed11"
   },
   "outputs": [],
   "source": [
    "def remove_noise(input_text):\n",
    "    text = re.sub('\\(talk\\)(.*)\\(utc\\)','',input_text)\n",
    "    text = text.split()\n",
    "    text = [re.sub('[\\d]+','',x) for x in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = remove_noise(X_train[i])\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = remove_noise(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "43173dca-e503-4bc9-8a82-b4b6a6fb806c",
    "_uuid": "d7a87d9b68c9e2a2a12d7046936bd850b64e55f3"
   },
   "outputs": [],
   "source": [
    "def replace_word(X):\n",
    "    repl = {\n",
    "        \"&lt;3\": \" good \",\":d\": \" good \",\":dd\": \" good \",\":p\": \" good \",\"8)\": \" good \",\":-)\": \" good \", \":)\": \" good \",\";)\": \" good \",\n",
    "        \"(-:\": \" good \",\"(:\": \" good \",\"yay!\": \" good \",\"yay\": \" good \",\"yaay\": \" good \",\"yaaay\": \" good \",\"yaaaay\": \" good \",\n",
    "        \"yaaaaay\": \" good \",\":/\": \" bad \",\":&gt;\": \" sad \",\":')\": \" sad \",\":-(\": \" bad \",\":(\": \" bad \", \":s\": \" bad \",\":-s\": \" bad \",\n",
    "        \"&lt;3\": \" heart \",\":d\": \" smile \",\":p\": \" smile \",\":dd\": \" smile \",\"8)\": \" smile \", \":-)\": \" smile \", \":)\": \" smile \",\n",
    "        \";)\": \" smile \",\"(-:\": \" smile \",\"(:\": \" smile \",\":/\": \" worry \",\":&gt;\": \" angry \", \":')\": \" sad \",\":-(\": \" sad \",\":(\": \" sad \",\n",
    "        \":s\": \" sad \", \":-s\": \" sad \",r\"\\br\\b\": \"are\",r\"\\bu\\b\": \"you\",r\"\\bhaha\\b\": \"ha\",r\"\\bhahaha\\b\": \"ha\",r\"\\bdon't\\b\": \"do not\",\n",
    "        r\"\\bdoesn't\\b\": \"does not\",r\"\\bdidn't\\b\": \"did not\",r\"\\bhasn't\\b\": \"has not\",r\"\\bhaven't\\b\": \"have not\",r\"\\bhadn't\\b\": \"had not\",\n",
    "        r\"\\bwon't\\b\": \"will not\",r\"\\bwouldn't\\b\": \"would not\",r\"\\bcan't\\b\": \"can not\",r\"\\bcannot\\b\": \"can not\",r\"\\bi'm\\b\": \"i am\",\n",
    "        \"m\": \"am\",\"r\": \"are\",\"u\": \"you\",\"haha\": \"ha\",\"hahaha\": \"ha\",\"don't\": \"do not\",\"doesn't\": \"does not\",\"didn't\": \"did not\",\n",
    "        \"hasn't\": \"has not\",\"haven't\": \"have not\",\"hadn't\": \"had not\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"can't\": \"can not\",\n",
    "        \"cannot\": \"can not\",\"i'm\": \"i am\",\"m\": \"am\",\"i'll\" : \"i will\",\"its\" : \"it is\",\"it's\" : \"it is\",\"'s\" : \" is\",\"that's\" : \"that is\",\n",
    "        \"weren't\" : \"were not\"\n",
    "    }\n",
    "    keys = repl.keys()\n",
    "    new_X = []\n",
    "    for i in X:\n",
    "        arr = str(i).split()\n",
    "        xx = \"\"\n",
    "        for j in arr:\n",
    "            j = str(j).lower()\n",
    "            if j[:4] == 'http' or j[:3] == 'www':\n",
    "                continue\n",
    "            if j in keys:\n",
    "                j = repl[j]\n",
    "            xx += j + \" \"\n",
    "        new_X.append(xx)\n",
    "    return new_X\n",
    "\n",
    "X_train = replace_word(X_train)\n",
    "X_test = replace_word(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "da409613-3688-4d2e-a072-f67dee02617b",
    "_uuid": "efad6a0ecd758a759f14287a69bfd9cafa8c8fb2"
   },
   "outputs": [],
   "source": [
    "max_features=200000\n",
    "max_senten_len=30\n",
    "max_senten_num=10\n",
    "embed_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "6f560b8e-fa4f-400f-b415-337ffc04f8cb",
    "_uuid": "799fd6b51a1415a4714ddaebf7d6435380c7c628"
   },
   "outputs": [],
   "source": [
    "def filt_sent(X,max_senten_num):\n",
    "    X_sent = []\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for paragraph in X:\n",
    "        raw = sent_tokenizer.tokenize(paragraph)\n",
    "        filt = []\n",
    "        min_sent_len = 5 if len(raw) <= 10 else 10\n",
    "        for sentence in raw:\n",
    "            if len(sentence.split()) >= min_sent_len and len(filt) < max_senten_num:\n",
    "                filt.append(sentence)\n",
    "        while len(filt) < max_senten_num:\n",
    "            filt.append('nosentence')\n",
    "        X_sent.append(filt)\n",
    "    return X_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "eed8323c-9d4e-4d39-b07a-dd91d915c2dc",
    "_uuid": "6d479f5e37f5990ea13f8cfbf69f4be9d3ff2f90"
   },
   "outputs": [],
   "source": [
    "X_train_sent = filt_sent(X_train ,max_senten_num)\n",
    "X_test_sent = filt_sent(X_test, max_senten_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "7a665c08-b4a9-4792-b40b-481b3da907e5",
    "_uuid": "b07e998ccedaf3aaaf4b4e67b207ad5490eb24f7"
   },
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "for i in range(len(X_train_sent)):\n",
    "        X_train_sent[i] = tok.texts_to_sequences(X_train_sent[i])\n",
    "        X_train_sent[i] = sequence.pad_sequences(X_train_sent[i],maxlen=max_senten_len)\n",
    "for i in range(len(X_test_sent)):\n",
    "        X_test_sent[i] = tok.texts_to_sequences(X_test_sent[i])\n",
    "        X_test_sent[i] = sequence.pad_sequences(X_test_sent[i],maxlen=max_senten_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9e57a7cb-c061-4361-bbe2-05c0486a3f18",
    "_uuid": "9488bc9d68dfd1fde1f99d23a9f1ed7b30ceb87f"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "e2490100-fc9c-4e46-ae84-7dfa65fcddba",
    "_uuid": "d56ad119931a971b2588355deb726a045764c9ad"
   },
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "43944725-1842-4093-a2fd-8701d09c9200",
    "_uuid": "269daf8e81503a014262929fbe984b4a15984df5"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "f27e0264-d863-450a-902a-395f265242f3",
    "_uuid": "78f033922530bb7687ca2a65179c9c17802753a3"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "fc02de57-3ce5-4f78-b66d-15538b57f90e",
    "_uuid": "19ffa659d304c2a406e566ca0abfb083dfad75a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=max_senten_len,\n",
    "                            weights=[embedding_matrix])\n",
    "\n",
    "word_input = Input(shape=(max_senten_len,), dtype='int32')\n",
    "word = embedding_layer(word_input)\n",
    "word = SpatialDropout1D(0.2)(word)\n",
    "word = Bidirectional(LSTM(128, return_sequences=True))(word)\n",
    "word_out = AttentionWithContext()(word)\n",
    "wordEncoder = Model(word_input, word_out)\n",
    "\n",
    "sente_input = Input(shape=(max_senten_num, max_senten_len), dtype='int32')\n",
    "sente = TimeDistributed(wordEncoder)(sente_input)\n",
    "sente = SpatialDropout1D(0.2)(sente)\n",
    "sente = Bidirectional(LSTM(128, return_sequences=True))(sente)\n",
    "sente = AttentionWithContext()(sente)\n",
    "preds = Dense(6, activation='sigmoid')(sente)\n",
    "model = Model(sente_input, preds)\n",
    "opt = Adam(clipnorm=5.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "90727db5-d597-4100-9e65-aba6e5d75791",
    "_uuid": "4861f4e238ebb34e1b1a09822ae52ed8081a5d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 10, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train_sent = np.asarray(X_train_sent)\n",
    "X_test_sent = np.asarray(X_test_sent)\n",
    "print('Shape of data tensor:', X_train_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58d507b5-2741-4f49-a547-253504370475",
    "_uuid": "4a8a9f1a9b4a16e7d4b0d6867758ffec0d9e2371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/3\n",
      "151592/151592 [==============================] - 2043s 13ms/step - loss: 0.0825 - acc: 0.9747 - val_loss: 0.0616 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97911, saving model to weights_base.best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.966155 \n",
      "\n",
      "Epoch 2/3\n",
      " 56576/151592 [==========>...................] - ETA: 21:13 - loss: 0.0584 - acc: 0.9799"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_train_sent, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "filepath=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "callbacks_list = [checkpoint, early, RocAuc]\n",
    "\n",
    "model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "265115a8-296e-4b67-a6fc-02d0a584b501",
    "_uuid": "f7377e50952ffb6cc14bab3b34442788864eed68",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_test,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5c703574-cf76-495f-b800-1fc6c9384ded",
    "_uuid": "ddc5afff4b22841fb184e32cc4b19a2225dec451",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n",
    "# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "78885dfc-dcbc-45f8-8be9-4ca9249f7986",
    "_uuid": "0f0aa330f8e577d28e1561be894156b4502dc06e",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
